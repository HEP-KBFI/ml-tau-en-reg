WARNING: Environment variable PYTHONPATH already has value [/home/norman/ml-tau-en-reg], will not forward new value [/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64/python2.7/site-packages:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib/python2.7/site-packages:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64/python2.7/site-packages:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib/python2.7/site-packages:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64/python2.7/site-packages:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib/python2.7/site-packages] from parent process environment
INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (540) bind mounts
2024-05-08 10:41:26.326984: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-08 10:41:27.125279: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-05-08 10:41:27.126960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-05-08 10:41:27.287798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-05-08 10:41:27.590595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-08 10:41:29.095224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<trainParticleTransformer>:
ZH_Htautau
[1/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_200254.parquet
[2/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_203213.parquet
[3/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_204507.parquet
[4/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_201924.parquet
[5/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_201761.parquet
[6/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_203635.parquet
[7/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_203654.parquet
[8/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_202316.parquet
[9/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_200414.parquet
[10/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_200501.parquet
Input data loaded
[1/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_200172.parquet
[2/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_203807.parquet
[3/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_203035.parquet
[4/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_204998.parquet
[5/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_201461.parquet
[6/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_202350.parquet
[7/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_204160.parquet
[8/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_204086.parquet
[9/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_204233.parquet
[10/10] Loading from /home/laurits/ENREG/ntuples/20240402_full_stats/ZH_Htautau/reco_p8_ee_ZH_Htautau_ecm380_201765.parquet
Input data loaded
Using device: cuda
Building model...
<ParticleTransformer::ParticleTransformer>:
 input_dim = 13
 num_classes = 16
cfg_block: {'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0.1, 'attn_dropout': 0.1, 'activation_dropout': 0.1, 'add_bias_kv': False, 'activation': 'gelu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True}
cfg_cls_block: {'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0, 'add_bias_kv': False, 'activation': 'gelu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True}
Finished building model:
ParticleTransformer(
  (trimmer): SequenceTrimmer()
  (embed): Embed(
    (input_bn): BatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (embed): Sequential(
      (0): LayerNorm((13,), eps=1e-05, elementwise_affine=True)
      (1): Linear(in_features=13, out_features=128, bias=True)
      (2): GELU(approximate='none')
      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (4): Linear(in_features=128, out_features=512, bias=True)
      (5): GELU(approximate='none')
      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (7): Linear(in_features=512, out_features=128, bias=True)
      (8): GELU(approximate='none')
    )
  )
  (pair_embed): PairEmbed(
    (embed): Sequential(
      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): GELU(approximate='none')
      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): GELU(approximate='none')
      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (9): GELU(approximate='none')
      (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
      (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (12): GELU(approximate='none')
    )
  )
  (blocks): ModuleList(
    (0-7): 8 x Block(
      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (act_dropout): Dropout(p=0.1, inplace=False)
      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (fc2): Linear(in_features=512, out_features=128, bias=True)
    )
  )
  (cls_blocks): ModuleList(
    (0-1): 2 x Block(
      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0, inplace=False)
      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (act_dropout): Dropout(p=0, inplace=False)
      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (fc2): Linear(in_features=512, out_features=128, bias=True)
    )
  )
  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (fc): Sequential(
    (0): Linear(in_features=128, out_features=16, bias=True)
  )
)
#trainable parameters = 2143732
Using AdamW optimizer.
Training for 100 epochs.
#batches(train) = 3
Starting training...
 current time: 2024-05-08 10:42:12.944215
Processing epoch #0
 current time: 2024-05-08 10:42:12.949657
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 6.527437  [512/1088]
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 5.116223  [1024/1088]
x shape torch.Size([25, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 4.269e-05
::::: VALIDATION LOOP :::::
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([17, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2043 Mb
GPU:
Wed May  8 10:42:17 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   30C    P2              59W / 215W |   2727MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2724MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #1
 current time: 2024-05-08 10:42:17.870172
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([18, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 3.722598  [512/1088]
x shape torch.Size([10, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 3.597258  [1024/1088]
x shape torch.Size([12, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 5.073e-05
::::: VALIDATION LOOP :::::
x shape torch.Size([23, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([12, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2197 Mb
GPU:
Wed May  8 10:42:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   31C    P2              59W / 215W |   2865MiB /  8192MiB |     18%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2862MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #2
 current time: 2024-05-08 10:42:20.678949
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([10, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 2.748662  [512/1088]
x shape torch.Size([23, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.896151  [1024/1088]
x shape torch.Size([18, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 6.402e-05
::::: VALIDATION LOOP :::::
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([21, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([25, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2197 Mb
GPU:
Wed May  8 10:42:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   31C    P2              56W / 215W |   2865MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2862MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #3
 current time: 2024-05-08 10:42:22.729793
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.659293  [512/1088]
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.443913  [1024/1088]
x shape torch.Size([12, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 8.242e-05
::::: VALIDATION LOOP :::::
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([23, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([12, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2198 Mb
GPU:
Wed May  8 10:42:24 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   31C    P2              59W / 215W |   2865MiB /  8192MiB |      1%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2862MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #4
 current time: 2024-05-08 10:42:24.821391
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([14, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.301642  [512/1088]
x shape torch.Size([11, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.374050  [1024/1088]
x shape torch.Size([13, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 1.057e-04
::::: VALIDATION LOOP :::::
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([23, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([24, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2198 Mb
GPU:
Wed May  8 10:42:26 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   31C    P2              59W / 215W |   2865MiB /  8192MiB |      3%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2862MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #5
 current time: 2024-05-08 10:42:26.848445
::::: TRAIN LOOP :::::
Training model
Finished training model
/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.271869  [512/1088]
x shape torch.Size([7, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
pred shape torch.Size([512, 16])
 Running loss: 1.552339  [1024/1088]
x shape torch.Size([14, 64, 128])
x shape torch.Size([64, 128])
output: torch.Size([64, 16])
pred shape torch.Size([64, 16])
 lr = 1.337e-04
::::: VALIDATION LOOP :::::
x shape torch.Size([25, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([22, 512, 128])
x shape torch.Size([512, 128])
output: torch.Size([512, 16])
x shape torch.Size([20, 85, 128])
x shape torch.Size([85, 128])
output: torch.Size([85, 16])
Found new best model :)
Saving best model to file ParticleTransformer_model_best.pt
Done.
System utilization:
 CPU-Util = 0.00%
 Memory-Usage = 2198 Mb
GPU:
Wed May  8 10:42:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070 ...    Off | 00000000:1A:00.0 Off |                  N/A |
| 20%   32C    P2              59W / 215W |   2865MiB /  8192MiB |      6%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3733625      C   /opt/conda/bin/python3                     2862MiB |
+---------------------------------------------------------------------------------------+

Processing epoch #6
 current time: 2024-05-08 10:42:28.902317
::::: TRAIN LOOP :::::
Training model
Finished training model
slurmstepd: error: *** JOB 54630166 ON gpu0 CANCELLED AT 2024-05-08T10:42:29 ***

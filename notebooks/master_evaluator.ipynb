{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master notebook for evaluating  performaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmplhep\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhep\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from enreg.tools import general as g\n",
    "from enreg.tools.metrics import (\n",
    "    regression_evaluator as re,\n",
    "    decay_mode_evaluator as dme,\n",
    "    tagger_evaluator as te\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluator:\n",
    "    def __init__(self, config_path=\"../enreg/config/\", config_name=\"benchmarking\"):\n",
    "        \"\"\"\n",
    "        Initialize the performance evaluator with configuration\n",
    "\n",
    "        Args:\n",
    "            config_path (str): Path to configuration directory\n",
    "            config_name (str): Name of the configuration file\n",
    "        \"\"\"\n",
    "        # Initialize configuration\n",
    "        with initialize(version_base=None, config_path=config_path, job_name=\"performance_eval\"):\n",
    "            self.cfg = compose(config_name=config_name)\n",
    "\n",
    "        # Use HEP style\n",
    "        hep.style.use(hep.style.CMS)\n",
    "\n",
    "        # Output directory for results\n",
    "        self.output_dir = \"/home/laurits/tmp/performance_evaluation\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def load_datasets(self, datasets):\n",
    "        \"\"\"\n",
    "        Load multiple datasets for evaluation\n",
    "        \n",
    "        Args:\n",
    "            datasets (dict): Dictionary of dataset paths\n",
    "        \n",
    "        Returns:\n",
    "            dict: Loaded datasets\n",
    "        \"\"\"\n",
    "        loaded_data = {}\n",
    "        for name, path in datasets.items():\n",
    "            loaded_data[name] = g.load_all_data(path)\n",
    "        return loaded_data\n",
    "\n",
    "    def prepare_regression_evaluators(self, datasets):\n",
    "        \"\"\"\n",
    "        Prepare regression evaluators for different models\n",
    "        \n",
    "        Args:\n",
    "            datasets (dict): Loaded datasets\n",
    "        \n",
    "        Returns:\n",
    "            dict: Regression evaluators\n",
    "        \"\"\"\n",
    "        evaluators = {\n",
    "            \"HPS\": re.RegressionEvaluator(\n",
    "                datasets['hps'].pred_pt, \n",
    "                datasets['hps'].true_pt, \n",
    "                self.cfg.metrics.regression, \n",
    "                \"zh\", \n",
    "                \"HPS\"\n",
    "            ),\n",
    "            \"RecoJet\": re.RegressionEvaluator(\n",
    "                datasets['recoJet'].reco_jet_pt, \n",
    "                datasets['recoJet'].gen_tau_pt, \n",
    "                self.cfg.metrics.regression, \n",
    "                \"zh\", \n",
    "                \"RecoJet\"\n",
    "            ),\n",
    "            # Add other model evaluators dynamically\n",
    "        }\n",
    "        \n",
    "        # Dynamically add other model evaluators\n",
    "        model_datasets = {\n",
    "            \"PT\": datasets['PT'].jet_regression,\n",
    "            \"LN\": datasets['LN'].jet_regression,\n",
    "            \"DS\": datasets['DS'].jet_regression\n",
    "        }\n",
    "        \n",
    "        for model_name, model_data in model_datasets.items():\n",
    "            evaluators[model_name] = re.RegressionEvaluator(\n",
    "                model_data.pred, \n",
    "                model_data.target, \n",
    "                self.cfg.metrics.regression, \n",
    "                \"zh\", \n",
    "                model_name\n",
    "            )\n",
    "        \n",
    "        return evaluators\n",
    "\n",
    "    def perform_regression_evaluation(self, evaluators):\n",
    "        \"\"\"\n",
    "        Perform regression evaluation and generate plots\n",
    "        \n",
    "        Args:\n",
    "            evaluators (dict): Regression evaluators\n",
    "        \"\"\"\n",
    "        # Combine and save regression results\n",
    "        multi_evaluator = re.RegressionMultiEvaluator(\n",
    "            self.output_dir, \n",
    "            self.cfg.metrics.regression, \n",
    "            \"zh\"\n",
    "        )\n",
    "        \n",
    "        # Adjust plot configurations if needed\n",
    "        self.cfg.metrics.regression.ratio_plot.resolution_plot.ylim = [0, 0.2]\n",
    "        self.cfg.metrics.regression.ratio_plot.response_plot.ylim = [0.96, 1.04]\n",
    "        \n",
    "        # Combine evaluators and save results\n",
    "        multi_evaluator.combine_results(list(evaluators.values()))\n",
    "        multi_evaluator.save()\n",
    "        \n",
    "        # Access specific plots\n",
    "        resolution_plot = multi_evaluator.resolution_lineplot.fig\n",
    "        response_plot = multi_evaluator.response_lineplot.fig\n",
    "\n",
    "    def perform_decay_mode_evaluation(self, datasets):\n",
    "        \"\"\"\n",
    "        Perform decay mode performance evaluation\n",
    "        \n",
    "        Args:\n",
    "            datasets (dict): Loaded datasets\n",
    "        \"\"\"\n",
    "        # Iterate through configured algorithms and samples\n",
    "        for algorithm, algo_info in self.cfg.metrics.dm_reconstruction.algorithms.items():\n",
    "            for signal_sample in self.cfg.metrics.dm_reconstruction.signal_samples:\n",
    "                # Load signal data\n",
    "                sig_data = g.load_all_data(\n",
    "                    os.path.join(algo_info.data_dir, f\"{signal_sample}.parquet\")\n",
    "                )\n",
    "                \n",
    "                # Create decay mode evaluator\n",
    "                evaluator = dme.DecayModeEvaluator(\n",
    "                    sig_data.dm_multiclass.pred, \n",
    "                    sig_data.dm_multiclass.target, \n",
    "                    os.path.join(self.output_dir, \"decay_mode_plots\"), \n",
    "                    signal_sample, \n",
    "                    algorithm\n",
    "                )\n",
    "                \n",
    "                # Save performance metrics\n",
    "                evaluator.save_performance()\n",
    "\n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"\n",
    "        Run complete performance evaluation workflow\n",
    "        \"\"\"\n",
    "        # Define datasets to load\n",
    "        dataset_paths = {\n",
    "            \"hps\": \"/home/laurits/HPS_recoCut0_ntuples/zh.parquet\",\n",
    "            \"recoJet\": \"/home/laurits/ntuples/20240924_lowered_recoPtCut/recoJet/zh.parquet\",\n",
    "            \"PT\": \"/home/laurits/ml-tau-en-reg/training-outputs/20240921_recoPtCut_removed_samples/v1/jet_regression/ParticleTransformer/zh_test.parquet\",\n",
    "            \"LN\": \"/home/laurits/ml-tau-en-reg/training-outputs/20240921_recoPtCut_removed_samples/v1/jet_regression/LorentzNet/zh_test.parquet\",\n",
    "            \"DS\": \"/home/laurits/ml-tau-en-reg/training-outputs/20240921_recoPtCut_removed_samples/v1/jet_regression/DeepSet/zh_test.parquet\"\n",
    "        }\n",
    "        \n",
    "        # Load datasets\n",
    "        datasets = self.load_datasets(dataset_paths)\n",
    "        \n",
    "        # Perform regression evaluation\n",
    "        regression_evaluators = self.prepare_regression_evaluators(datasets)\n",
    "        self.perform_regression_evaluation(regression_evaluators)\n",
    "        \n",
    "        # Perform decay mode evaluation\n",
    "        self.perform_decay_mode_evaluation(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    evaluator = PerformanceEvaluator()\n",
    "    evaluator.run_full_evaluation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

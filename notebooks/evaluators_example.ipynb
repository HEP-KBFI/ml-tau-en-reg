{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f981c-2f5c-4f8d-9156-3262b612b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=\"enreg/config/\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3411-7c5a-4223-af79-eb5124d67166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enreg.tools.metrics import decay_mode_evaluator as dme\n",
    "from enreg.tools.metrics import regression_evaluator as re\n",
    "from enreg.tools.metrics import tagger_evaluator as te\n",
    "from enreg.tools import general as g\n",
    "import os\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e49f7-562e-4bdb-9d8d-8b0db6afbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.comparison_samples = ['zh_test', 'z_test', 'qq_test']\n",
    "algorithms = [\"DeepSet\"]\n",
    "tasks = [\"binary_classification\", \"jet_regression\", \"dm_multiclass\"]\n",
    "signal_samples = ['zh_test', 'z_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87a1db-a2fb-40cc-9dd0-6b64d0228bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {sample: g.load_all_data(os.path.join(cfg.base_ntuple_path, sample + \".parquet\")) for sample in cfg.comparison_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f4aa6-f747-489c-8d73-a480edd0e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"binary_classification\"\n",
    "evaluators = []\n",
    "for algorithm in algorithms:\n",
    "    base_path = os.path.join(\"/home/laurits/ml-tau-en-reg/training-outputs/Trainings/v1\", task, algorithm)\n",
    "    bkg_data = g.load_all_data(os.path.join(base_path, \"qq_test.parquet\"))\n",
    "    for signal_sample in signal_samples:\n",
    "        sig_info_data = data[signal_sample]\n",
    "        bkg_info_data = data['qq_test']\n",
    "        sig_data = g.load_all_data(os.path.join(base_path, signal_sample + \".parquet\"))\n",
    "        \n",
    "        evaluator = te.TaggerEvaluator(\n",
    "            signal_predictions=sig_data.binary_classification.pred,\n",
    "            signal_truth=sig_data.binary_classification.target,\n",
    "            signal_gen_tau_p4=sig_info_data.gen_jet_tau_p4s,\n",
    "            signal_reco_jet_p4=sig_info_data.reco_jet_p4s,\n",
    "            bkg_predictions=bkg_data.binary_classification.pred,\n",
    "            bkg_truth=bkg_data.binary_classification.target,\n",
    "            bkg_gen_jet_p4=bkg_info_data.gen_jet_p4s,\n",
    "            bkg_reco_jet_p4=bkg_info_data.reco_jet_p4s,\n",
    "            cfg=cfg.metrics.classifier,\n",
    "            sample=signal_sample,\n",
    "            algorithm=algorithm\n",
    "        )\n",
    "        evaluators.append(evaluator)\n",
    "\n",
    "tme = te.TaggerMultiEvaluator(\"output_plots_cls\", cfg.metrics.classifier)\n",
    "tme.combine_results(evaluators)\n",
    "tme.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222915e2-edef-4821-82a9-cc98f1ee4ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = \"dm_multiclass\"\n",
    "for algorithm in algorithms:\n",
    "    base_path = os.path.join(\"/home/laurits/ml-tau-en-reg/training-outputs/Trainings/v1\", task, algorithm)\n",
    "    for signal_sample in signal_samples:\n",
    "        sig_info_data = data[signal_sample]\n",
    "        sig_data = g.load_all_data(os.path.join(base_path, signal_sample + \".parquet\"))\n",
    "\n",
    "        output_dir = \"output_plots_dm\"\n",
    "        evaluator = dme.DecayModeEvaluator(sig_data.dm_multiclass.pred, sig_data.dm_multiclass.target, output_dir, signal_sample, algorithm)\n",
    "        evaluator.save_performance()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e872e6f-5f98-4aab-bd5d-15e007c72f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"jet_regression\"\n",
    "evaluators = []\n",
    "for algorithm in algorithms:\n",
    "    base_path = os.path.join(\"/home/laurits/ml-tau-en-reg/training-outputs/Trainings/v1\", task, algorithm)\n",
    "    for signal_sample in signal_samples:\n",
    "        sig_info_data = data[signal_sample]\n",
    "        sig_data = g.load_all_data(os.path.join(base_path, signal_sample + \".parquet\"))\n",
    "\n",
    "        evaluator = re.RegressionEvaluator(sig_data.jet_regression.pred, sig_data.jet_regression.target, cfg.metrics.regression, signal_sample.split(\"_\")[0], algorithm)\n",
    "        evaluators.append(evaluator)\n",
    "output_dir = \"output_plots_regression\"\n",
    "rme = re.RegressionMultiEvaluator(output_dir, cfg.metrics.regression, signal_sample.split(\"_\")[0])\n",
    "rme.combine_results(evaluators)\n",
    "rme.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import torch\n",
    "import vector\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import gabbro.plotting.utils as plot_utils\n",
    "from gabbro.data.loading import read_jetclass_file\n",
    "from gabbro.models.vqvae import VQVAELightning\n",
    "from gabbro.plotting.feature_plotting import plot_features\n",
    "from gabbro.utils.arrays import ak_select_and_preprocess, ak_subtract\n",
    "\n",
    "plot_utils.set_mpl_style()\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# free, avail = torch.cuda.mem_get_info()\n",
    "# clear cuda memory\n",
    "# print(free / avail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_p4(p4_obj):\n",
    "    return vector.awk(\n",
    "        ak.zip(\n",
    "            {\n",
    "                \"mass\": p4_obj.tau,\n",
    "                \"x\": p4_obj.x,\n",
    "                \"y\": p4_obj.y,\n",
    "                \"z\": p4_obj.z,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def p4s_relative_to_axis(p4s):\n",
    "    axis = ak.sum(p4s, axis=1)\n",
    "    return ak.Array(\n",
    "        {\n",
    "            \"part_pt\": p4s.pt,\n",
    "            \"part_etarel\": p4s.deltaeta(axis),\n",
    "            \"part_phirel\": p4s.deltaphi(axis),\n",
    "            \"part_mass\": p4s.mass,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def future_file_to_ak_arrays(\n",
    "    file_path: str,\n",
    "    n_load: int,\n",
    "    pp_dict: dict,\n",
    "    pp_dict_cuts: dict,\n",
    "    vqvae_model,\n",
    "):\n",
    "    \"\"\"Loads a file from the future dataset and returns the original and tokenized arrays.\"\"\"\n",
    "    data = ak.from_parquet(file_path)\n",
    "    p4s = to_p4(data.reco_cand_p4s[:n_load])\n",
    "    p4s_rel = p4s_relative_to_axis(p4s)\n",
    "    charge = data.reco_cand_charge[:n_load]\n",
    "    pdg = data.reco_cand_pdg[:n_load]\n",
    "\n",
    "    x_ak = ak.Array(\n",
    "        {\n",
    "            \"part_etarel\": p4s_rel.part_etarel,\n",
    "            \"part_phirel\": p4s_rel.part_phirel,\n",
    "            \"part_pt\": p4s_rel.part_pt,\n",
    "            \"part_mass\": p4s_rel.part_mass,\n",
    "            \"part_charge\": charge,\n",
    "            \"part_isElectron\": ak.where(abs(pdg) == 11, 1, 0),\n",
    "            \"part_isMuon\": ak.where(abs(pdg) == 13, 1, 0),\n",
    "            \"part_isPhoton\": ak.where(abs(pdg) == 22, 1, 0),\n",
    "            \"part_isChargedHadron\": ak.where(abs(pdg) == 211, 1, 0)\n",
    "            + ak.where(abs(pdg) == 321, 1, 0)\n",
    "            + ak.where(abs(pdg) == 2212, 1, 0),\n",
    "            \"part_isNeutralHadron\": ak.where(abs(pdg) == 130, 1, 0)\n",
    "            + ak.where(abs(pdg) == 2112, 1, 0)\n",
    "            + ak.where(abs(pdg) == 0, 1, 0),\n",
    "        }\n",
    "    )\n",
    "    # apply the cuts that were used in the VQVAE training\n",
    "    x_ak_with_cuts = ak_select_and_preprocess(x_ak, pp_dict_cuts)\n",
    "\n",
    "    # tokenize\n",
    "    x_ak_tokenized = vqvae_model.tokenize_ak_array(\n",
    "        ak_arr=x_ak_with_cuts,\n",
    "        pp_dict=pp_dict,\n",
    "        batch_size=512,\n",
    "        pad_length=128,\n",
    "    )\n",
    "    # reconstruct\n",
    "    x_ak_with_cuts_reco = vqvae_model.reconstruct_ak_tokens(\n",
    "        tokens_ak=x_ak_tokenized[\"part_token_id\"],\n",
    "        pp_dict=pp_dict,\n",
    "        batch_size=512,\n",
    "        pad_length=128,\n",
    "    )\n",
    "    return x_ak_with_cuts, x_ak_with_cuts_reco\n",
    "\n",
    "\n",
    "def jetclass_file_to_ak_arrays(\n",
    "    jetclass_file_path: str,\n",
    "    n_load: int,\n",
    "    pp_dict: dict,\n",
    "    pp_dict_cuts: dict,\n",
    "    vqvae_model,\n",
    "):\n",
    "    x_ak, _, _ = read_jetclass_file(\n",
    "        filepath=jetclass_file_path,\n",
    "        particle_features=pp_dict.keys(),\n",
    "        jet_features=None,\n",
    "        labels=None,\n",
    "        n_load=n_load,\n",
    "    )\n",
    "    # apply the cuts that were used in the VQVAE training\n",
    "    x_ak_with_cuts = ak_select_and_preprocess(x_ak, pp_dict_cuts)[:, :128]\n",
    "    # tokenize\n",
    "    x_ak_tokenized = vqvae_model.tokenize_ak_array(\n",
    "        ak_arr=x_ak_with_cuts,\n",
    "        pp_dict=pp_dict,\n",
    "        batch_size=512,\n",
    "        pad_length=128,\n",
    "    )\n",
    "    # reconstruct\n",
    "    x_ak_with_cuts_reco = vqvae_model.reconstruct_ak_tokens(\n",
    "        tokens_ak=x_ak_tokenized[\"part_token_id\"],\n",
    "        pp_dict=pp_dict,\n",
    "        batch_size=512,\n",
    "        pad_length=128,\n",
    "    )\n",
    "    return x_ak_with_cuts, x_ak_with_cuts_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the tokenizer model from checkpoint, and also get the feature_dict from the config ---\n",
    "\n",
    "\n",
    "# ckpt_path = \"/data/dust/user/birkjosc/beegfs/datasets/jetclass_tokenized/2024-02-19_20-54-01_nonfissile_defect_a56f_all_types/model_ckpt.ckpt\"\n",
    "ckpt_path = \"/home/laurits/ml-tau-en-reg/enreg/omnijet_alpha/checkpoints/vqvae_32000_tokens_p3_mass_pid/model_ckpt.ckpt\"\n",
    "vqvae_model = VQVAELightning.load_from_checkpoint(ckpt_path)\n",
    "vqvae_model.eval()  # important\n",
    "\n",
    "cfg = OmegaConf.load(Path(ckpt_path).parent / \"config.yaml\")\n",
    "pp_dict = OmegaConf.to_container(cfg.data.dataset_kwargs_common.feature_dict)\n",
    "for item in pp_dict:\n",
    "    print(item, pp_dict[item])\n",
    "\n",
    "pp_dict_cuts = {}\n",
    "for feat_name in pp_dict:\n",
    "    if pp_dict[feat_name] is None:\n",
    "        pp_dict_cuts[feat_name] = {\"larger_than\": None, \"smaller_than\": None}\n",
    "        continue\n",
    "    pp_dict_cuts[feat_name] = {\n",
    "        \"larger_than\": pp_dict[feat_name].get(\"larger_than\", None),\n",
    "        \"smaller_than\": pp_dict[feat_name].get(\"smaller_than\", None),\n",
    "    }\n",
    "\n",
    "for item in pp_dict_cuts:\n",
    "    print(item, pp_dict_cuts[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files from JetClass and Future dataset and convert them to our awkward arrays for plotting\n",
    "\n",
    "n_load = 1000\n",
    "data_dir = \"/scratch/persistent/laurits/ml-tau/20240924_lowered_recoPtCut/\"\n",
    "zh_path = os.path.join(data_dir, \"zh_train.parquet\")\n",
    "z_path = os.path.join(data_dir, \"z_train.parquet\")\n",
    "qq_path = os.path.join(data_dir, \"qq_train.parquet\")\n",
    "zh_ak_particles_original, zh_ak_particles_reco = future_file_to_ak_arrays(\n",
    "    zh_path, n_load, pp_dict, pp_dict_cuts, vqvae_model\n",
    ")\n",
    "z_ak_particles_original, z_ak_particles_reco = future_file_to_ak_arrays(\n",
    "    z_path, n_load, pp_dict, pp_dict_cuts, vqvae_model\n",
    ")\n",
    "qq_ak_particles_original, qq_ak_particles_reco = future_file_to_ak_arrays(\n",
    "    qq_path, n_load, pp_dict, pp_dict_cuts, vqvae_model\n",
    ")\n",
    "\n",
    "jetclass_file_path_qg = (\n",
    "    \"/scratch/persistent/joosep/jetclass/test_20M/ZJetsToNuNu_100.root\"\n",
    ")\n",
    "jetclass_file_path_tbqq = (\n",
    "    \"/scratch/persistent/joosep/jetclass/test_20M/TTBar_100.root\"\n",
    ")\n",
    "jetclass_qg_ak_particles_original, jetclass_qg_ak_particles_reco = jetclass_file_to_ak_arrays(\n",
    "    jetclass_file_path_qg, n_load, pp_dict, pp_dict_cuts, vqvae_model\n",
    ")\n",
    "jetclass_tbqq_ak_particles_original, jetclass_tbqq_ak_particles_reco = jetclass_file_to_ak_arrays(\n",
    "    jetclass_file_path_tbqq, n_load, pp_dict, pp_dict_cuts, vqvae_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that each particle has exactly one pid\n",
    "z_ak_particles_pid_sum = (\n",
    "    zh_ak_particles_original.part_isElectron\n",
    "    + zh_ak_particles_original.part_isMuon\n",
    "    + zh_ak_particles_original.part_isPhoton\n",
    "    + zh_ak_particles_original.part_isChargedHadron\n",
    "    + zh_ak_particles_original.part_isNeutralHadron\n",
    ")\n",
    "sum(\n",
    "    ak.sum(z_ak_particles_pid_sum, axis=1) == ak.num(zh_ak_particles_original.part_etarel, axis=1)\n",
    ") == n_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot difference\n",
    "labels_diff = {\n",
    "    \"part_pt\": \"Particle $p_T^\\\\mathrm{reco} - p_T^\\\\mathrm{original}$ [GeV]\",\n",
    "    \"part_etarel\": \"Particle $\\\\Delta\\\\eta^\\\\mathrm{reco} - \\\\Delta\\\\eta^\\\\mathrm{original}$ \",\n",
    "    \"part_phirel\": \"Particle $\\\\Delta\\\\phi^\\\\mathrm{reco} - \\\\Delta\\\\phi^\\\\mathrm{original}$ \",\n",
    "    \"part_mass\": \"Particle $m^\\\\mathrm{reco} - m^\\\\mathrm{original} [GeV]$ \",\n",
    "    # \"part_charge\": \"Particle charge difference\",\n",
    "    # \"part_isElectron\": \"isElectron difference\",\n",
    "    # \"part_isMuon\": \"isMuon difference\",\n",
    "    # \"part_isPhoton\": \"isPhoton difference\",\n",
    "    # \"part_isChargedHadron\": \"isChargedHadron difference\",\n",
    "    # \"part_isNeutralHadron\": \"isNeutralHadron difference\",\n",
    "}\n",
    "fig, axarr = plot_features(\n",
    "    ak_array_dict={\n",
    "        \"$\\\\mathrm{Fu}\\\\tau\\\\mathrm{ure}$ $ZH$\": ak_subtract(\n",
    "            zh_ak_particles_reco, zh_ak_particles_original\n",
    "        ),\n",
    "        # \"$\\\\mathrm{Fu}\\\\tau\\\\mathrm{ure}$ $Z$\": ak_subtract(\n",
    "        #     z_ak_particles_reco, z_ak_particles_original\n",
    "        # ),\n",
    "        \"$\\\\mathrm{Fu}\\\\tau\\\\mathrm{ure}$ $qq$\": ak_subtract(\n",
    "            qq_ak_particles_reco, qq_ak_particles_original\n",
    "        ),\n",
    "        \"JetClass $q/g$\": ak_subtract(\n",
    "            jetclass_qg_ak_particles_reco, jetclass_qg_ak_particles_original\n",
    "        ),\n",
    "        \"JetClass $t\\\\to bqq'$\": ak_subtract(\n",
    "            jetclass_tbqq_ak_particles_reco, jetclass_tbqq_ak_particles_original\n",
    "        ),\n",
    "    },\n",
    "    names=labels_diff,\n",
    "    ax_rows=2,\n",
    "    bins_dict={\n",
    "        \"part_pt\": np.linspace(-5, 5, 100),\n",
    "        \"part_etarel\": np.linspace(-0.1, 0.1, 100),\n",
    "        \"part_phirel\": np.linspace(-0.1, 0.1, 100),\n",
    "        \"part_mass\": np.linspace(-0.05, 0.05, 100),\n",
    "        # \"part_charge\": np.linspace(-1.5, 1.5, 4),\n",
    "        # \"part_isElectron\": np.linspace(-1.5, 1.5, 4),\n",
    "        # \"part_isMuon\": np.linspace(-1.5, 1.5, 4),\n",
    "        # \"part_isPhoton\": np.linspace(-1.5, 1.5, 4),\n",
    "        # \"part_isChargedHadron\": np.linspace(-1.5, 1.5, 4),\n",
    "        # \"part_isNeutralHadron\": np.linspace(-1.5, 1.5, 4),\n",
    "    },\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper left\",\n",
    "        \"ncol\": 2,\n",
    "        \"fontsize\": 10,\n",
    "    },\n",
    "    ax_size=(4.2, 2),\n",
    "    decorate_ax_kwargs={\"yscale\": 2.1},\n",
    "    ylabel=\"Normalized\",\n",
    "    # legend_only_on=0,\n",
    ")\n",
    "fig.savefig('/home/laurits/tmp/feature_differences.pdf')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "\n",
    "def visualize_confusion_matrix(\n",
    "        histogram: np.array,\n",
    "        categories: list,\n",
    "        cmap: str = \"Greys\",\n",
    "        bin_text_color: str = \"r\",\n",
    "        y_label: str = \"Predicted decay modes\",\n",
    "        x_label: str = \"True decay modes\",\n",
    "        figsize: tuple = (12, 12),\n",
    "):\n",
    "    \"\"\"Plots the confusion matrix for the classification task. Confusion\n",
    "    matrix functions has the categories in the other way in order to have the\n",
    "    truth on the x-axis.\n",
    "    Args:\n",
    "        histogram : np.array\n",
    "            Histogram produced by the sklearn.metrics.confusion_matrix.\n",
    "        categories : list\n",
    "            Category labels in the correct order.\n",
    "        cmap : str\n",
    "            [default: \"gray\"] The colormap to be used.\n",
    "        bin_text_color : str\n",
    "            [default: \"r\"] The color of the text on bins.\n",
    "        y_label : str\n",
    "            [default: \"Predicted\"] The label for the y-axis.\n",
    "        x_label : str\n",
    "            [default: \"Truth\"] The label for the x-axis.\n",
    "        figsize : tuple\n",
    "            The size of the figure drawn.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    xbins = ybins = np.arange(len(categories) + 1)\n",
    "    tick_values = np.arange(len(categories)) + 0.5\n",
    "    hep.hist2dplot(histogram, xbins, ybins, flow=None, ax=ax, cbar=False)\n",
    "    plt.xticks(tick_values, categories, fontsize=14, rotation=0)\n",
    "    plt.yticks(tick_values + 0.2, categories, fontsize=14, rotation=90, va=\"center\")\n",
    "    plt.xlabel(f\"{x_label}\", fontdict={\"size\": 14})\n",
    "    plt.ylabel(f\"{y_label}\", fontdict={\"size\": 14})\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "    for i in range(len(ybins) - 1):\n",
    "        for j in range(len(xbins) - 1):\n",
    "            bin_value = histogram.T[i, j]\n",
    "            ax.text(\n",
    "                float(xbins[j] + 0.5),\n",
    "                float(ybins[i] + 0.5),\n",
    "                f\"{bin_value:.2f}\",\n",
    "                color=bin_text_color,\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "    return fig, ax\n",
    "\n",
    "def get_pid_classes(original_particles, reco_particles):\n",
    "    original_concatenate = ak.concatenate([\n",
    "        ak.unflatten(ak.flatten(original_particles.part_isElectron),counts=1),\n",
    "        ak.unflatten(ak.flatten(original_particles.part_isMuon),counts=1),\n",
    "        ak.unflatten(ak.flatten(original_particles.part_isPhoton),counts=1),\n",
    "        ak.unflatten(ak.flatten(original_particles.part_isChargedHadron),counts=1),\n",
    "        ak.unflatten(ak.flatten(original_particles.part_isNeutralHadron),counts=1)],\n",
    "        axis=-1)\n",
    "    reco_concatenate = ak.concatenate([\n",
    "        ak.unflatten(ak.flatten(reco_particles.part_isElectron),counts=1),\n",
    "        ak.unflatten(ak.flatten(reco_particles.part_isMuon),counts=1),\n",
    "        ak.unflatten(ak.flatten(reco_particles.part_isPhoton),counts=1),\n",
    "        ak.unflatten(ak.flatten(reco_particles.part_isChargedHadron),counts=1),\n",
    "        ak.unflatten(ak.flatten(reco_particles.part_isNeutralHadron),counts=1)],\n",
    "        axis=-1)\n",
    "    reco_classes = ak.argmax(reco_concatenate, axis=-1)\n",
    "    original_classes = ak.argmax(original_concatenate, axis=-1)\n",
    "    return original_classes, reco_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tau dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_charges_original = ak.flatten(zh_ak_particles_original.part_charge).to_numpy()\n",
    "ft_charges_reco = ak.round(ak.flatten(zh_ak_particles_reco.part_charge)).to_numpy()\n",
    "ft_charges_reco[ft_charges_reco < -1] = -1\n",
    "\n",
    "normalized_confusion_matrix = metrics.confusion_matrix(ft_charges_original, ft_charges_reco, normalize=\"pred\")\n",
    "fig, ax = visualize_confusion_matrix(\n",
    "    histogram=normalized_confusion_matrix,\n",
    "    categories=[\"-1\", \"0\", \"+1\"],\n",
    "    cmap=None,\n",
    "    bin_text_color=\"r\",\n",
    "    y_label=\"Predicted charge\",\n",
    "    x_label=\"True charge\",\n",
    "    figsize=(5, 5)\n",
    ")\n",
    "fig.savefig('/home/laurits/tmp/ft_charges.pdf')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classes_ft, reco_classes_ft = get_pid_classes(zh_ak_particles_original, zh_ak_particles_reco)\n",
    "normalized_confusion_matrix = metrics.confusion_matrix(original_classes_ft, reco_classes_ft, normalize=\"pred\")\n",
    "fig, ax = visualize_confusion_matrix(\n",
    "    histogram=normalized_confusion_matrix,\n",
    "    categories=[r\"$e^{\\pm}$\",r\"$\\mu^{\\pm}$\", r\"$\\gamma$\", r\"$h^{\\pm}$\", r\"$h^{0}$\"],\n",
    "    cmap=None,\n",
    "    bin_text_color=\"r\",\n",
    "    y_label=\"Predicted PID\",\n",
    "    x_label=\"True PID\",\n",
    "    figsize=(5, 5)\n",
    ")\n",
    "fig.savefig('/home/laurits/tmp/ft_pid.pdf')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JetClass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_charges_original = ak.flatten(jetclass_tbqq_ak_particles_original.part_charge).to_numpy()\n",
    "jc_charges_reco = ak.round(ak.flatten(jetclass_tbqq_ak_particles_reco.part_charge)).to_numpy()\n",
    "jc_charges_reco[jc_charges_reco < -1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_confusion_matrix = metrics.confusion_matrix(jc_charges_original, jc_charges_reco, normalize=\"pred\")\n",
    "fig, ax = visualize_confusion_matrix(\n",
    "    histogram=normalized_confusion_matrix,\n",
    "    categories=[\"-1\", \"0\", \"+1\"],\n",
    "    cmap=None,\n",
    "    bin_text_color=\"r\",\n",
    "    y_label=\"Predicted charge\",\n",
    "    x_label=\"True charge\",\n",
    "    figsize=(5, 5)\n",
    ")\n",
    "fig.savefig('/home/laurits/tmp/jc_charges.pdf')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classes_jc, reco_classes_jc = get_pid_classes(jetclass_tbqq_ak_particles_original, jetclass_tbqq_ak_particles_reco)\n",
    "jc_normalized_confusion_matrix = metrics.confusion_matrix(original_classes_jc, reco_classes_jc, normalize=\"pred\")\n",
    "fig, ax = visualize_confusion_matrix(\n",
    "    histogram=jc_normalized_confusion_matrix,\n",
    "    categories=[r\"$e^{\\pm}$\",r\"$\\mu^{\\pm}$\", r\"$\\gamma$\", r\"$h^{\\pm}$\", r\"$h^{0}$\"],\n",
    "    cmap=None,\n",
    "    bin_text_color=\"r\",\n",
    "    y_label=\"Predicted PID\",\n",
    "    x_label=\"True PID\",\n",
    "    figsize=(5, 5)\n",
    ")\n",
    "fig.savefig('/home/laurits/tmp/jc_pid.pdf')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot plain distributions of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots(figsize=(16,9))\n",
    "ax.hist(ak.flatten(zh_ak_particles_reco.part_mass), density=True, label=\"Reco\")\n",
    "ax.hist(ak.flatten(zh_ak_particles_original.part_mass), density=True, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.xscale('symlog')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16,9))\n",
    "ax.hist(ak.flatten(qq_ak_particles_reco.part_mass), density=True, label=\"Reco\")\n",
    "ax.hist(ak.flatten(qq_ak_particles_original.part_mass), density=True, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.xscale('symlog')\n",
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.sum(jetclass_qg_ak_particles_original.part_mass < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16,9))\n",
    "ax.hist(ak.flatten(jetclass_qg_ak_particles_reco.part_mass), density=True, label=\"Reco\")\n",
    "ax.hist(ak.flatten(nii.part_mass), density=True, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.xscale('symlog')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16,9))\n",
    "ax.hist(ak.flatten(jetclass_tbqq_ak_particles_reco.part_mass), density=True, label=\"Reco\")\n",
    "ax.hist(ak.flatten(jetclass_tbqq_ak_particles_original.part_mass), density=True, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.xscale('symlog')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
